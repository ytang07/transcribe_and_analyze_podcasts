{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install deepgram-sdk requests matplotlib youtube_dl ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first download stuff from youtube\n",
    "import youtube_dl\n",
    "\n",
    "# change this variable to change what you download\n",
    "vids = ['https://www.youtube.com/watch?v=VBPTFlpv31k&list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4&index=1']\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    # change this to change where you download it to\n",
    "    'outtmpl': './lex/audio/%(title)s.mp3',\n",
    "}\n",
    "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(vids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we use deepgram to get a transcription\n",
    "from deepgram import Deepgram\n",
    "import asyncio, json, os\n",
    "\n",
    "from config import dg_key\n",
    "dg = Deepgram(dg_key)\n",
    "\n",
    "options = {\n",
    "    \"diarize\": True,\n",
    "    \"punctuate\": True,\n",
    "    \"paragraphs\": True,\n",
    "    \"model\": 'general',\n",
    "    \"tier\": 'enhanced'\n",
    "}\n",
    "\n",
    "async def main():\n",
    "    podcasts = os.listdir(\"./lex/audio\")\n",
    "    for podcast in podcasts:\n",
    "        if \"Bishop Robert Barron\" in podcast:\n",
    "            continue\n",
    "        print(podcast)\n",
    "        with open(f\"lex/audio/{podcast}\", \"rb\") as audio:\n",
    "            source = {\"buffer\": audio, \"mimetype\":'audio/mp3'}\n",
    "            res = await dg.transcription.prerecorded(source, options)\n",
    "            with open(f\"lex/transcripts/{podcast[:-4]}.json\", \"w\") as transcript:\n",
    "                # print(transcript)\n",
    "                json.dump(res, transcript)\n",
    "    return\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print transcripts into a script and separate speakers\n",
    "import json\n",
    "import os\n",
    "\n",
    "# create transcripts\n",
    "def create_transcripts():\n",
    "    for filename in os.listdir(\"lex/transcripts_unenhanced\"):\n",
    "        # if \"Joe Rogan\" in filename or \"Bishop Robert Barron\" in filename:\n",
    "        #     continue\n",
    "        with open(f\"lex/transcripts_unenhanced/{filename}\", \"r\") as file:\n",
    "            transcript = json.load(file)\n",
    "        paragraphs = transcript[\"results\"][\"channels\"][0][\"alternatives\"][0][\"paragraphs\"]\n",
    "        # words = transcript[\"results\"][\"channels\"][0][\"alternatives\"][0][\"words\"]\n",
    "        print(paragraphs['transcript'])\n",
    "        with open(f\"lex/pretty_scripts/{filename[:-5]}.txt\", \"w\") as f:\n",
    "            for line in paragraphs['transcript']:\n",
    "                f.write(line)\n",
    "# create_transcripts()\n",
    "\n",
    "# separate transcripts by speaker\n",
    "# label speakers by printing first lines by the speaker\n",
    "# coalesce them into one file\n",
    "def assign_speakers():\n",
    "    for filename in os.listdir(\"lex/pretty_scripts\"):\n",
    "        print(f\"Current File: {filename}\")\n",
    "        with open(f\"lex/pretty_scripts/{filename}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        spoken = []\n",
    "        names = []\n",
    "        for line in lines:\n",
    "            if line.startswith(\"Speaker \"):\n",
    "                if line[0:9] in spoken:\n",
    "                    continue\n",
    "                print(line)\n",
    "                name = input(\"Who is the Speaker?\")\n",
    "                if len(name) <= 1:\n",
    "                    continue\n",
    "                spoken.append(line[:9])\n",
    "                names.append(name)\n",
    "        print(spoken)\n",
    "        print(names)\n",
    "        filedata = \"\\n\".join(lines)\n",
    "        print(filedata)\n",
    "        for speaker, name in zip(spoken, names):\n",
    "            filedata = filedata.replace(speaker, name)\n",
    "        with open(f\"lex/pretty_scripts/{filename}\", \"w\") as f:\n",
    "            f.write(filedata)\n",
    "\n",
    "# assign_speakers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import json\n",
    "import os\n",
    "\n",
    "def divide_times():\n",
    "    times = {}\n",
    "    for filename in os.listdir(\"lex/transcripts_unenhanced\"):\n",
    "        print(f\"Current filename: {filename}\")\n",
    "        with open(f\"lex/transcripts_unenhanced/{filename}\", \"r\") as file:\n",
    "            transcript = json.load(file)\n",
    "        paragraphs = transcript[\"results\"][\"channels\"][0][\"alternatives\"][0][\"paragraphs\"][\"paragraphs\"]\n",
    "        speaker_times = {}\n",
    "        assigned_speakers = {}\n",
    "        for paragraph in paragraphs:\n",
    "            len_spoken = paragraph[\"end\"]-paragraph[\"start\"]\n",
    "            speaker = paragraph[\"speaker\"]\n",
    "            if speaker in assigned_speakers:\n",
    "                speaker = assigned_speakers[speaker]\n",
    "            else:\n",
    "                print(paragraph)\n",
    "                name = input(\"Who is the speaker?\")\n",
    "                assigned_speakers[speaker] = name\n",
    "                speaker = name\n",
    "            if speaker in speaker_times:\n",
    "                speaker_times[speaker] += len_spoken\n",
    "            else:\n",
    "                speaker_times[speaker] = len_spoken\n",
    "        times[filename] = speaker_times\n",
    "    with open(\"./lex/time_speaking.json\", \"w\") as f:\n",
    "        json.dump(times, f)\n",
    "\n",
    "divide_times()\n",
    "\n",
    "def words_said():\n",
    "    word_split = {}\n",
    "    for filename in os.listdir(\"lex/pretty_scripts\"):\n",
    "        print(f\"Current filename: {filename}\")\n",
    "        with open(f\"lex/pretty_scripts/{filename}\", \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "        cur_speaker = None\n",
    "        file_word_split = {}\n",
    "        for line in lines:\n",
    "            if \":\" in line:\n",
    "                sep = line.split(\":\")\n",
    "                cur_speaker = sep[0]\n",
    "                if cur_speaker in file_word_split:\n",
    "                    file_word_split[cur_speaker] += len(sep[1])\n",
    "                else:\n",
    "                    file_word_split[cur_speaker] = len(sep[1])\n",
    "        word_split[filename] = file_word_split\n",
    "    with open(\"./lex/word_split.json\", \"w\") as f:\n",
    "        json.dump(word_split, f)\n",
    "\n",
    "words_said()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# plot speaking times bar charts\n",
    "def vis_time():\n",
    "    with open(\"./lex/time_speaking.json\", \"r\") as f:\n",
    "        time_dict = json.load(f)\n",
    "    labels = []\n",
    "    lex = []\n",
    "    guest = []\n",
    "    for podcast in time_dict.values():\n",
    "        for entry in podcast:\n",
    "            if \"Lex\" in entry:\n",
    "                lex.append(podcast[entry])\n",
    "            else:\n",
    "                guest.append(podcast[entry])\n",
    "                labels.append(entry)\n",
    "    print(labels)\n",
    "    print(lex)\n",
    "    print(guest)\n",
    "    width = 0.3\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(labels, lex, width, label=\"Lex\")\n",
    "    ax.bar(labels, guest, width, bottom=lex, label=\"Guest\")\n",
    "    ax.set_ylabel(\"Time Spent Speaking\")\n",
    "    ax.set_title(\"Lex vs Guests Speaking Time\")\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"./lex/time_speaking.png\", pad_inches=1)\n",
    "    plt.show()\n",
    "\n",
    "vis_time()\n",
    "\n",
    "# plot graph for words said\n",
    "def vis_words():\n",
    "    with open(\"./lex/word_split.json\", \"r\") as f:\n",
    "        time_dict = json.load(f)\n",
    "    labels = []\n",
    "    lex = []\n",
    "    guest = []\n",
    "    for podcast in time_dict.values():\n",
    "        for entry in podcast:\n",
    "            if \"Lex\" in entry:\n",
    "                lex.append(podcast[entry])\n",
    "            else:\n",
    "                guest.append(podcast[entry])\n",
    "                labels.append(entry)\n",
    "    print(labels)\n",
    "    print(lex)\n",
    "    print(guest)\n",
    "    width = 0.3\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(labels, lex, width, label=\"Lex\")\n",
    "    ax.bar(labels, guest, width, bottom=lex, label=\"Guest\")\n",
    "    ax.set_ylabel(\"Words Said\")\n",
    "    ax.set_title(\"Lex vs Guests Number of Words Said\")\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"./lex/words_said.png\", pad_inches=1)\n",
    "    plt.show()\n",
    "\n",
    "vis_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcp/ner/summaries\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from config import textapi_key\n",
    "\n",
    "headers = {\n",
    "    \"apikey\": textapi_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "def mcp(text: str, filename: str):\n",
    "    print(f\"Number of Characters: {len(text)}\")\n",
    "    sentences = text.split(\".\")\n",
    "    print(f\"Number of Sentences: {len(sentences)}\")\n",
    "    words = len(text.split(\" \"))\n",
    "    print(f\"Number of Words: {words}\")\n",
    "    texts = []\n",
    "    sents = 0\n",
    "    while sents < len(sentences):\n",
    "        texts.append(\" \".join(sentences[sents:sents+1500 if sents + 1500 < len(sentences) else len(sentences)]))\n",
    "        sents += 1500\n",
    "    mcps = []\n",
    "    for text in texts:\n",
    "        body = {\n",
    "            \"text\": text,\n",
    "            \"num_phrases\": 5\n",
    "        }\n",
    "        start = time.time()\n",
    "        res = requests.post(url=\"https://app.thetextapi.com/text/most_common_phrases\", headers=headers, json=body)\n",
    "        print(f\"Time elapsed: {time.time() - start} seconds\")\n",
    "        mcps.append(json.loads(res.text)[\"most common phrases\"])\n",
    "    print(mcps)\n",
    "    with open(f\"lex/most_common_phrases/{filename}.txt\", \"w\") as file:\n",
    "        for mcp in mcps:\n",
    "            for phrase in mcp:\n",
    "                file.write(phrase+\"\\n\")\n",
    "\n",
    "def ner(text: str, filename: str):\n",
    "    print(f\"Number of Characters: {len(text)}\")\n",
    "    sentences = text.split(\".\")\n",
    "    print(f\"Number of Sentences: {len(sentences)}\")\n",
    "    words = len(text.split(\" \"))\n",
    "    print(f\"Number of Words: {words}\")\n",
    "    texts = []\n",
    "    sents = 0\n",
    "    while sents < len(sentences):\n",
    "        texts.append(\" \".join(sentences[sents:sents+1500 if sents + 1500 < len(sentences) else len(sentences)]))\n",
    "        sents += 1500\n",
    "    ners = []\n",
    "    for text in texts:\n",
    "        body = {\n",
    "            \"text\": text\n",
    "        }\n",
    "        words = len(text.split(\" \"))\n",
    "        print(f\"Processing 1500 Sentences, {words} Words\")\n",
    "        start = time.time()\n",
    "        res = requests.post(url=\"https://app.thetextapi.com/text/ner\", headers=headers, json=body)\n",
    "        print(f\"Time elapsed: {time.time() - start} seconds\")\n",
    "        ners.append(json.loads(res.text)[\"ner\"])\n",
    "    with open(f\"lex/ner/{filename}.txt\", \"w\") as file:\n",
    "        for ner in ners:\n",
    "            for phrase in ner:\n",
    "                for word in phrase:\n",
    "                    file.write(word+\" \")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "def summarize(text: str, filename: str):\n",
    "    print(f\"Title: {filename}\")\n",
    "    print(f\"Number of Characters: {len(text)}\")\n",
    "    sentences = text.split(\".\")\n",
    "    print(f\"Number of Sentences: {len(sentences)}\")\n",
    "    words = len(text.split(\" \"))\n",
    "    print(f\"Number of Words: {words}\")\n",
    "    texts = []\n",
    "    sents = 0\n",
    "    while sents < len(sentences):\n",
    "        texts.append(\" \".join(sentences[sents:sents+1500 if sents + 1500 < len(sentences) else len(sentences)]))\n",
    "        sents += 1500\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        body = {\n",
    "            \"text\": text\n",
    "        }\n",
    "        start = time.time()\n",
    "        res = requests.post(url=\"https://app.thetextapi.com/text/summarize\", headers=headers, json=body)\n",
    "        print(f\"Time elapsed: {time.time() - start} seconds\")\n",
    "        summaries.append(json.loads(res.text)[\"summary\"])\n",
    "    # print(summaries)\n",
    "    with open(f\"lex/summarize/{filename}.txt\", \"w\") as file:\n",
    "        for summary in summaries:\n",
    "            file.write(summary)\n",
    "\n",
    "def main():\n",
    "    for filename in os.listdir(\"lex/transcripts_unenhanced\"):\n",
    "        with open(f\"lex/transcripts_unenhanced/{filename}\", \"r\") as file:\n",
    "            transcript = json.load(file)\n",
    "        text = transcript[\"results\"][\"channels\"][0][\"alternatives\"][0][\"transcript\"]\n",
    "        # mcp(text, filename[:-5])\n",
    "        # ner(text, filename[:-5])\n",
    "        summarize(text, filename[:-5])\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate most common phrases\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "from text_analysis import headers # gotta comment out main() in text_analytics to do this\n",
    "\n",
    "# Re-run MCP on both\n",
    "def nlp():\n",
    "    for filename in os.listdir(\"lex/pretty_scripts\"):\n",
    "        print(f\"Current File: {filename}\")\n",
    "        with open(f\"lex/pretty_scripts/{filename}\", \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        separated_speakers = dict()\n",
    "        for line in lines:\n",
    "            if \":\" in line:\n",
    "                speaker_sep = line.split(\":\")\n",
    "                if speaker_sep[0][1:] in separated_speakers.keys():\n",
    "                    separated_speakers[speaker_sep[0][1:]] += speaker_sep[1]\n",
    "                else:\n",
    "                    separated_speakers[speaker_sep[0][1:]] = speaker_sep[1]\n",
    "        for speaker, spoken in separated_speakers.items():\n",
    "            body = {\n",
    "                \"text\": spoken,\n",
    "                \"num_phrases\": 5\n",
    "            }\n",
    "            res = requests.post(\"https://app.thetextapi.com/text/most_common_phrases\", headers=headers, json=body)\n",
    "            mcp = json.loads(res.text)[\"most common phrases\"]\n",
    "            with open(f\"lex/most_common_phrases/{speaker} in {filename}\", \"w\") as f:\n",
    "                for entry in mcp:\n",
    "                    f.write(f\"{entry}\\n\")\n",
    "\n",
    "nlp()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('dg_plus_yt': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ccb4f31e81b19d196bbac066caca5d222f2bb20938f55af05f3ca51e34eca69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
